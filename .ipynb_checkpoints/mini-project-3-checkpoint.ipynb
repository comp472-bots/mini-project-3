{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e25637",
   "metadata": {},
   "source": [
    "# 2.2 Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f24938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.8% 1659.9/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_word2vec_google_news_300 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1fc46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "def documentation(model):\n",
    "    # Let's print all the methods of the model\n",
    "    print(dir(model))\n",
    "\n",
    "documentation(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29611f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some testing to explore how different methods work!\n",
      "\n",
      "Let's test the following question_word to find the closest synonym: 'enormously'\n",
      "Given the following options: ['appropriately', 'uniquely', 'tremendously', 'decidedly']\n",
      "\n",
      "Using the 'model.most_similar_to_given' method, we compute the following: 'tremendously'\n",
      "\n",
      "Let's see the cosine between the 'enormously' and 'tremendously' using the 'model.similarity' method!\n",
      "The cosine similarity between enormously and tremendously is: 0.8185791969299316\n",
      "\n",
      "If we compute the cosine similarity manually, we should also achieve the same result: 0.8185791969299316\n"
     ]
    }
   ],
   "source": [
    "def testing(model): \n",
    "    print(\"Let's do some testing to explore how different methods work!\\n\")\n",
    "    \n",
    "    question_word = \"enormously\"\n",
    "    options = [\"appropriately\", \"uniquely\", \"tremendously\", \"decidedly\"]\n",
    "    print(f\"Let's test the following question_word to find the closest synonym: '{question_word}'\")\n",
    "    print(f\"Given the following options: {options}\")\n",
    "    \n",
    "    # Let's test the \"most_similar_to_given\" method!\n",
    "    answer_word = model.most_similar_to_given(question_word, options)\n",
    "    print(f\"\\nUsing the 'model.most_similar_to_given' method, we compute the following: '{answer_word}'\")\n",
    "    \n",
    "    # Let's test the \"model.similarity\" method!\n",
    "    print(f\"\\nLet's see the cosine between the '{question_word}' and '{answer_word}' using the 'model.similarity' method!\")\n",
    "    cos_sim = model.similarity(question_word, answer_word)\n",
    "    print(f\"The cosine similarity between {question_word} and {answer_word} is: {cos_sim}\")\n",
    "\n",
    "    # Let's make sure the answer computed above correctly calculates the cosine similarity! \n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    v1 = model.get_vector(question_word)\n",
    "    v2 = model.get_vector(answer_word)\n",
    "    cos_sim = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    print(f\"\\nIf we compute the cosine similarity manually, we should also achieve the same result: {cos_sim}\")\n",
    "\n",
    "testing(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c35e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,ceremonies,wrong\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,impress,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,rear,correct\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,apathetically,correct\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,mastery,correct\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,happiness,guess\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,alike,correct\n",
      "figure,solve,solve,correct\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: word2vec-google-news-300\n",
      "(b) The size of the vocabulary: 3000000\n",
      "(c) The number of correct labels (call this 'C'): 70\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 79\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8860759493670886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8860759493670886"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "class Labels(Enum):\n",
    "    GUESS = \"guess\"\n",
    "    CORRECT = \"correct\"\n",
    "    WRONG = \"wrong\"\n",
    "    \n",
    "def random_guess(options):\n",
    "    random_index = random.randint(0, len(options) - 1)\n",
    "    return options[random_index]\n",
    "\n",
    "def print_analysis(model, model_name, num_correct, num_wrong):\n",
    "    print(f\"\\n\")\n",
    "    print(f\"(a) Model name: {model_name}\")\n",
    "    print(f\"(b) The size of the vocabulary: {len(model)}\")\n",
    "    print(f\"(c) The number of correct labels (call this 'C'): {num_correct}\")\n",
    "    print(f\"(d) The number of questions the Model answered without guessing (call this 'V' ): {num_correct + num_wrong}\")\n",
    "    print(f\"(e) the accuracy of the model (i.e. C/V): {num_correct/(num_correct + num_wrong)}\")\n",
    "\n",
    "def compute_most_similar_term(model, question_word, options):\n",
    "    similarity = None\n",
    "    guess_word = None\n",
    "    for option in options:\n",
    "        try:\n",
    "            current = model.similarity(question_word, option)\n",
    "            if not similarity or current > similarity:\n",
    "                similarity = current\n",
    "                guess_word = option\n",
    "        except KeyError:\n",
    "            pass     \n",
    "    return (guess_word, similarity)\n",
    "\n",
    "def compute_synonym(model, row):\n",
    "    question_word = row[0]\n",
    "    answer_word = row[1]\n",
    "    options = row[2:]\n",
    "    \n",
    "    guess_word, similarity, label = None, None, None\n",
    "    \n",
    "    # Let's check if the question word is defined in the vocabulary\n",
    "    try:\n",
    "        question_word_vector = model.get_vector(question_word)\n",
    "    except KeyError:\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "        return (question_word, answer_word, guess_word, similarity, Labels.GUESS)\n",
    "    \n",
    "    # If the question word is defined, we can proceed with finding the most similar term\n",
    "    (guess_word, similarity) = compute_most_similar_term(model, question_word, options)\n",
    "    \n",
    "    if guess_word == answer_word:\n",
    "        label = Labels.CORRECT\n",
    "    elif not guess_word:\n",
    "        # If none of the options are in the vocabulary, then it's a guess\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "    else:\n",
    "        label = Labels.WRONG\n",
    "    \n",
    "    return (question_word, answer_word, guess_word, similarity, label)\n",
    "            \n",
    "def compute_synonyms(model, model_name):\n",
    "    with open('synonyms.csv', newline='') as csvfile:\n",
    "\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        # Skip the first row of the csv\n",
    "        next(reader)\n",
    "        \n",
    "        num_correct, num_guess, num_wrong = 0, 0, 0\n",
    "\n",
    "        details_file = open(f\"{model_name}-details.csv\", mode='w', newline=\"\")\n",
    "        details_writer = csv.writer(details_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        analysis_file = open(f\"analysis.csv\", mode='a', newline=\"\")\n",
    "        analysis_writer = csv.writer(analysis_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for row in reader:\n",
    "            question_word, answer_word, guess_word, similarity, label = None, None, None, None, None\n",
    "\n",
    "            (question_word, answer_word, guess_word, similarity, label) = compute_synonym(model, row)\n",
    "            \n",
    "            if label == Labels.CORRECT:\n",
    "                num_correct += 1\n",
    "            elif label == Labels.GUESS:\n",
    "                num_guess += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            output = f\"{question_word},{answer_word},{guess_word},{label.value}\"\n",
    "            print(output)\n",
    "\n",
    "            details_writer.writerow([question_word, answer_word, guess_word, label.value])\n",
    "         \n",
    "        performance = (num_correct/(num_correct + num_wrong)) * 100\n",
    "        \n",
    "        analysis_writer.writerow([model_name, len(model), num_correct, (num_correct + num_wrong), performance])\n",
    "        print_analysis(model, model_name, num_correct, num_wrong)\n",
    "        \n",
    "        details_file.close()\n",
    "        analysis_file.close()\n",
    "        \n",
    "        return performance\n",
    "\n",
    "def clear_file(file_name):\n",
    "    analysis_file = open(f\"{file_name}\", mode='w')\n",
    "    analysis_file.truncate(0)\n",
    "    analysis_file.close()\n",
    "\n",
    "clear_file(\"analysis.csv\")\n",
    "clear_file(\"word2vec-google-news-300-details.csv\")\n",
    "compute_synonyms(model_word2vec_google_news_300, \"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0fd8d",
   "metadata": {},
   "source": [
    "# 2.3 Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1424b",
   "metadata": {},
   "source": [
    "1. 2 new models from different corpora but same embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5e6094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the various models available for use in the Gensim library\n",
    "info = api.info()\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e087a874",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/qj3fdrhn4bl0x8qcjr71q42w0000gn/T/ipykernel_51519/2743817246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Let's use a model from Twitter corpora with embedding size of 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_word2vec_glove_twitter_100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-twitter-100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/glove-twitter-100/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-100.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         )\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             )\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 100\n",
    "model_word2vec_glove_wiki_100 = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Let's use a model from Twitter corpora with embedding size of 100\n",
    "model_word2vec_glove_twitter_100 = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec40c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,complex,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-100\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 65\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8125\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,jurisdictions,wrong\n",
      "haphazardly,randomly,densely,wrong\n",
      "prominent,conspicuous,mysterious,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,endurance,correct\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,repeated,wrong\n",
      "constantly,continually,instantly,wrong\n",
      "issues,subjects,benefits,wrong\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,trade,wrong\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,election,wrong\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,relative,wrong\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,humorously,wrong\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unrecognized,wrong\n",
      "peculiarly,uniquely,partly,guess\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,overtly,wrong\n",
      "physician,doctor,nurse,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,materials,wrong\n",
      "unlikely,improbable,different,wrong\n",
      "halfheartedly,apathetically,unconventionally,guess\n",
      "annals,chronicles,trails,wrong\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,prominent,wrong\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,binding,wrong\n",
      "tranquillity,peacefulness,weariness,wrong\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,consistently,wrong\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,equitable,wrong\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,profit,wrong\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,valuable,wrong\n",
      "fashion,manner,craze,wrong\n",
      "marketed,sold,diluted,wrong\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,permanently,wrong\n",
      "\n",
      "\n",
      "(a) Model name: glove-twitter-100\n",
      "(b) The size of the vocabulary: 1193514\n",
      "(c) The number of correct labels (call this 'C'): 39\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 78\n",
      "(e) the accuracy of the model (i.e. C/V): 0.5\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_100 = compute_synonyms(model_word2vec_glove_wiki_100, \"glove-wiki-gigaword-100\")\n",
    "performance_twitter_100 = compute_synonyms(model_word2vec_glove_twitter_100, \"glove-twitter-100\")\n",
    "\n",
    "performances_list = [performance_wiki_100, performance_twitter_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4693ec24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's compare the results of models from different corpora but same embedding size\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "corpora_list = ['Twitter', 'Wikipedia']\n",
    "\n",
    "# Human gold standard performance retrieved from class results\n",
    "human_standard_perforance = 0.85\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.bar(corpora_list, performances_list, color = 'powderblue', width = 0.7, align = 'center')\n",
    "\n",
    "plt.xlabel(\"Corpora\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.title(\"Performance of Different Corpora with same Embedding Size\")\n",
    "plt.axhline(human_standard_perforance, color = 'orangered')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80f3c4",
   "metadata": {},
   "source": [
    "2. 2 new models from the same corpus but different embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9ce200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 50\n",
    "model_word2vec_glove_wiki_50 = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Let's also use a model from Wikipedia corpora with embedding size of 300 instead\n",
    "model_word2vec_glove_wiki_300 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22d3d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,circulate,correct\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,pharmacist,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,isolated,wrong\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,unpopular,wrong\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,isolate,wrong\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,repeatedly,wrong\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,better,wrong\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-50\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 57\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.7125\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,supply,correct\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,circulate,correct\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,curved,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,ended,correct\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-300\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 71\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8875\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_50 = compute_synonyms(model_word2vec_glove_wiki_50, \"glove-wiki-gigaword-50\")\n",
    "performance_wiki_300 = compute_synonyms(model_word2vec_glove_wiki_300, \"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcd13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
