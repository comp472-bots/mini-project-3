{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e25637",
   "metadata": {},
   "source": [
    "# 2.2 Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f24938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domen\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_word2vec_google_news_300 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1fc46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "def documentation(model):\n",
    "    # Let's print all the methods of the model\n",
    "    print(dir(model))\n",
    "\n",
    "documentation(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29611f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some testing to explore how different methods work!\n",
      "\n",
      "Let's test the following question_word to find the closest synonym: 'enormously'\n",
      "Given the following options: ['appropriately', 'uniquely', 'tremendously', 'decidedly']\n",
      "\n",
      "Using the 'model.most_similar_to_given' method, we compute the following: 'tremendously'\n",
      "\n",
      "Let's see the cosine between the 'enormously' and 'tremendously' using the 'model.similarity' method!\n",
      "The cosine similarity between enormously and tremendously is: 0.8185791969299316\n",
      "\n",
      "If we compute the cosine similarity manually, we should also achieve the same result: 0.8185791969299316\n"
     ]
    }
   ],
   "source": [
    "def testing(model): \n",
    "    print(\"Let's do some testing to explore how different methods work!\\n\")\n",
    "    \n",
    "    question_word = \"enormously\"\n",
    "    options = [\"appropriately\", \"uniquely\", \"tremendously\", \"decidedly\"]\n",
    "    print(f\"Let's test the following question_word to find the closest synonym: '{question_word}'\")\n",
    "    print(f\"Given the following options: {options}\")\n",
    "    \n",
    "    # Let's test the \"most_similar_to_given\" method!\n",
    "    answer_word = model.most_similar_to_given(question_word, options)\n",
    "    print(f\"\\nUsing the 'model.most_similar_to_given' method, we compute the following: '{answer_word}'\")\n",
    "    \n",
    "    # Let's test the \"model.similarity\" method!\n",
    "    print(f\"\\nLet's see the cosine between the '{question_word}' and '{answer_word}' using the 'model.similarity' method!\")\n",
    "    cos_sim = model.similarity(question_word, answer_word)\n",
    "    print(f\"The cosine similarity between {question_word} and {answer_word} is: {cos_sim}\")\n",
    "\n",
    "    # Let's make sure the answer computed above correctly calculates the cosine similarity! \n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    v1 = model.get_vector(question_word)\n",
    "    v2 = model.get_vector(answer_word)\n",
    "    cos_sim = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    print(f\"\\nIf we compute the cosine similarity manually, we should also achieve the same result: {cos_sim}\")\n",
    "\n",
    "testing(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c35e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,ceremonies,wrong\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,impress,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,rear,correct\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,apathetically,correct\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,mastery,correct\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,harshness,guess\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,alike,correct\n",
      "figure,solve,solve,correct\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: word2vec-google-news-300\n",
      "(b) The size of the vocabulary: 3000000\n",
      "(c) The number of correct labels (call this 'C'): 70\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 79\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "class Labels(Enum):\n",
    "    GUESS = \"guess\"\n",
    "    CORRECT = \"correct\"\n",
    "    WRONG = \"wrong\"\n",
    "    \n",
    "def random_guess(options):\n",
    "    random_index = random.randint(0, len(options) - 1)\n",
    "    return options[random_index]\n",
    "\n",
    "def print_analysis(model, model_name, num_correct, num_wrong):\n",
    "    print(f\"\\n\")\n",
    "    print(f\"(a) Model name: {model_name}\")\n",
    "    print(f\"(b) The size of the vocabulary: {len(model)}\")\n",
    "    print(f\"(c) The number of correct labels (call this 'C'): {num_correct}\")\n",
    "    print(f\"(d) The number of questions the Model answered without guessing (call this 'V' ): {num_correct + num_wrong}\")\n",
    "    print(f\"(e) the accuracy of the model (i.e. C/V): {num_correct/(num_correct + num_wrong)}\")\n",
    "\n",
    "def compute_most_similar_term(model, question_word, options):\n",
    "    similarity = None\n",
    "    guess_word = None\n",
    "    for option in options:\n",
    "        try:\n",
    "            current = model.similarity(question_word, option)\n",
    "            if not similarity or current > similarity:\n",
    "                similarity = current\n",
    "                guess_word = option\n",
    "        except KeyError:\n",
    "            pass     \n",
    "    return (guess_word, similarity)\n",
    "\n",
    "def compute_synonym(model, row):\n",
    "    question_word = row[0]\n",
    "    answer_word = row[1]\n",
    "    options = row[2:]\n",
    "    \n",
    "    guess_word, similarity, label = None, None, None\n",
    "    \n",
    "    # Let's check if the question word is defined in the vocabulary\n",
    "    try:\n",
    "        question_word_vector = model.get_vector(question_word)\n",
    "    except KeyError:\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "        return (question_word, answer_word, guess_word, similarity, Labels.GUESS)\n",
    "    \n",
    "    # If the question word is defined, we can proceed with finding the most similar term\n",
    "    (guess_word, similarity) = compute_most_similar_term(model, question_word, options)\n",
    "    \n",
    "    if guess_word == answer_word:\n",
    "        label = Labels.CORRECT\n",
    "    elif not guess_word:\n",
    "        # If none of the options are in the vocabulary, then it's a guess\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "    else:\n",
    "        label = Labels.WRONG\n",
    "    \n",
    "    return (question_word, answer_word, guess_word, similarity, label)\n",
    "            \n",
    "def compute_synonyms(model, model_name):\n",
    "    with open('synonyms.csv', newline='') as csvfile:\n",
    "\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        # Skip the first row of the csv\n",
    "        next(reader)\n",
    "        \n",
    "        num_correct, num_guess, num_wrong = 0, 0, 0\n",
    "\n",
    "        details_file = open(f\"{model_name}-details.csv\", mode='w', newline=\"\")\n",
    "        details_writer = csv.writer(details_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        analysis_file = open(f\"analysis.csv\", mode='a', newline=\"\")\n",
    "        analysis_writer = csv.writer(analysis_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for row in reader:\n",
    "            question_word, answer_word, guess_word, similarity, label = None, None, None, None, None\n",
    "\n",
    "            (question_word, answer_word, guess_word, similarity, label) = compute_synonym(model, row)\n",
    "            \n",
    "            if label == Labels.CORRECT:\n",
    "                num_correct += 1\n",
    "            elif label == Labels.GUESS:\n",
    "                num_guess += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            output = f\"{question_word},{answer_word},{guess_word},{label.value}\"\n",
    "            print(output)\n",
    "\n",
    "            details_writer.writerow([question_word, answer_word, guess_word, label.value])\n",
    "        \n",
    "        analysis_writer.writerow([model_name, len(model), num_correct, (num_correct + num_wrong), num_correct/(num_correct + num_wrong)])\n",
    "        print_analysis(model, model_name, num_correct, num_wrong)\n",
    "        \n",
    "        details_file.close()\n",
    "        analysis_file.close()\n",
    "\n",
    "def clear_file(file_name):\n",
    "    analysis_file = open(f\"{file_name}\", mode='w')\n",
    "    analysis_file.truncate(0)\n",
    "    analysis_file.close()\n",
    "\n",
    "clear_file(\"analysis.csv\")\n",
    "clear_file(\"word2vec-google-news-300-details.csv\")\n",
    "compute_synonyms(model_word2vec_google_news_300, \"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0fd8d",
   "metadata": {},
   "source": [
    "# 2.3 Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885641c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c9990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
