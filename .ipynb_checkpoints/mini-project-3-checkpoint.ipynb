{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e25637",
   "metadata": {},
   "source": [
    "# 2.2 Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f24938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.8% 1659.9/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_word2vec_google_news_300 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1fc46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "def documentation(model):\n",
    "    # Let's print all the methods of the model\n",
    "    print(dir(model))\n",
    "\n",
    "documentation(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29611f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some testing to explore how different methods work!\n",
      "\n",
      "Let's test the following question_word to find the closest synonym: 'enormously'\n",
      "Given the following options: ['appropriately', 'uniquely', 'tremendously', 'decidedly']\n",
      "\n",
      "Using the 'model.most_similar_to_given' method, we compute the following: 'tremendously'\n",
      "\n",
      "Let's see the cosine between the 'enormously' and 'tremendously' using the 'model.similarity' method!\n",
      "The cosine similarity between enormously and tremendously is: 0.8185791969299316\n",
      "\n",
      "If we compute the cosine similarity manually, we should also achieve the same result: 0.8185791969299316\n"
     ]
    }
   ],
   "source": [
    "def testing(model): \n",
    "    print(\"Let's do some testing to explore how different methods work!\\n\")\n",
    "    \n",
    "    question_word = \"enormously\"\n",
    "    options = [\"appropriately\", \"uniquely\", \"tremendously\", \"decidedly\"]\n",
    "    print(f\"Let's test the following question_word to find the closest synonym: '{question_word}'\")\n",
    "    print(f\"Given the following options: {options}\")\n",
    "    \n",
    "    # Let's test the \"most_similar_to_given\" method!\n",
    "    answer_word = model.most_similar_to_given(question_word, options)\n",
    "    print(f\"\\nUsing the 'model.most_similar_to_given' method, we compute the following: '{answer_word}'\")\n",
    "    \n",
    "    # Let's test the \"model.similarity\" method!\n",
    "    print(f\"\\nLet's see the cosine between the '{question_word}' and '{answer_word}' using the 'model.similarity' method!\")\n",
    "    cos_sim = model.similarity(question_word, answer_word)\n",
    "    print(f\"The cosine similarity between {question_word} and {answer_word} is: {cos_sim}\")\n",
    "\n",
    "    # Let's make sure the answer computed above correctly calculates the cosine similarity! \n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    v1 = model.get_vector(question_word)\n",
    "    v2 = model.get_vector(answer_word)\n",
    "    cos_sim = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    print(f\"\\nIf we compute the cosine similarity manually, we should also achieve the same result: {cos_sim}\")\n",
    "\n",
    "testing(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c35e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,ceremonies,wrong\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,impress,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,rear,correct\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,apathetically,correct\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,mastery,correct\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,happiness,guess\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,alike,correct\n",
      "figure,solve,solve,correct\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: word2vec-google-news-300\n",
      "(b) The size of the vocabulary: 3000000\n",
      "(c) The number of correct labels (call this 'C'): 70\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 79\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8860759493670886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8860759493670886"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "class Labels(Enum):\n",
    "    GUESS = \"guess\"\n",
    "    CORRECT = \"correct\"\n",
    "    WRONG = \"wrong\"\n",
    "    \n",
    "def random_guess(options):\n",
    "    random_index = random.randint(0, len(options) - 1)\n",
    "    return options[random_index]\n",
    "\n",
    "def print_analysis(model, model_name, num_correct, num_wrong):\n",
    "    print(f\"\\n\")\n",
    "    print(f\"(a) Model name: {model_name}\")\n",
    "    print(f\"(b) The size of the vocabulary: {len(model)}\")\n",
    "    print(f\"(c) The number of correct labels (call this 'C'): {num_correct}\")\n",
    "    print(f\"(d) The number of questions the Model answered without guessing (call this 'V' ): {num_correct + num_wrong}\")\n",
    "    print(f\"(e) the accuracy of the model (i.e. C/V): {num_correct/(num_correct + num_wrong)}\")\n",
    "\n",
    "def compute_most_similar_term(model, question_word, options):\n",
    "    similarity = None\n",
    "    guess_word = None\n",
    "    for option in options:\n",
    "        try:\n",
    "            current = model.similarity(question_word, option)\n",
    "            if not similarity or current > similarity:\n",
    "                similarity = current\n",
    "                guess_word = option\n",
    "        except KeyError:\n",
    "            pass     \n",
    "    return (guess_word, similarity)\n",
    "\n",
    "def compute_synonym(model, row):\n",
    "    question_word = row[0]\n",
    "    answer_word = row[1]\n",
    "    options = row[2:]\n",
    "    \n",
    "    guess_word, similarity, label = None, None, None\n",
    "    \n",
    "    # Let's check if the question word is defined in the vocabulary\n",
    "    try:\n",
    "        question_word_vector = model.get_vector(question_word)\n",
    "    except KeyError:\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "        return (question_word, answer_word, guess_word, similarity, Labels.GUESS)\n",
    "    \n",
    "    # If the question word is defined, we can proceed with finding the most similar term\n",
    "    (guess_word, similarity) = compute_most_similar_term(model, question_word, options)\n",
    "    \n",
    "    if guess_word == answer_word:\n",
    "        label = Labels.CORRECT\n",
    "    elif not guess_word:\n",
    "        # If none of the options are in the vocabulary, then it's a guess\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "    else:\n",
    "        label = Labels.WRONG\n",
    "    \n",
    "    return (question_word, answer_word, guess_word, similarity, label)\n",
    "            \n",
    "def compute_synonyms(model, model_name):\n",
    "    with open('synonyms.csv', newline='') as csvfile:\n",
    "\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        # Skip the first row of the csv\n",
    "        next(reader)\n",
    "        \n",
    "        num_correct, num_guess, num_wrong = 0, 0, 0\n",
    "\n",
    "        details_file = open(f\"{model_name}-details.csv\", mode='w', newline=\"\")\n",
    "        details_writer = csv.writer(details_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        analysis_file = open(f\"analysis.csv\", mode='a', newline=\"\")\n",
    "        analysis_writer = csv.writer(analysis_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for row in reader:\n",
    "            question_word, answer_word, guess_word, similarity, label = None, None, None, None, None\n",
    "\n",
    "            (question_word, answer_word, guess_word, similarity, label) = compute_synonym(model, row)\n",
    "            \n",
    "            if label == Labels.CORRECT:\n",
    "                num_correct += 1\n",
    "            elif label == Labels.GUESS:\n",
    "                num_guess += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            output = f\"{question_word},{answer_word},{guess_word},{label.value}\"\n",
    "            print(output)\n",
    "\n",
    "            details_writer.writerow([question_word, answer_word, guess_word, label.value])\n",
    "         \n",
    "        performance = num_correct/(num_correct + num_wrong)\n",
    "        \n",
    "        analysis_writer.writerow([model_name, len(model), num_correct, (num_correct + num_wrong), performance])\n",
    "        print_analysis(model, model_name, num_correct, num_wrong)\n",
    "        \n",
    "        details_file.close()\n",
    "        analysis_file.close()\n",
    "        \n",
    "        return performance\n",
    "\n",
    "def clear_file(file_name):\n",
    "    analysis_file = open(f\"{file_name}\", mode='w')\n",
    "    analysis_file.truncate(0)\n",
    "    analysis_file.close()\n",
    "\n",
    "clear_file(\"analysis.csv\")\n",
    "clear_file(\"word2vec-google-news-300-details.csv\")\n",
    "compute_synonyms(model_word2vec_google_news_300, \"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0fd8d",
   "metadata": {},
   "source": [
    "# 2.3 Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fee528",
   "metadata": {},
   "source": [
    "1. 2 new models from different corpora but same embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcec200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the various models available for use in the Gensim library\n",
    "info = api.info()\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d143e50b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/qj3fdrhn4bl0x8qcjr71q42w0000gn/T/ipykernel_51519/2743817246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Let's use a model from Twitter corpora with embedding size of 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_word2vec_glove_twitter_100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-twitter-100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/glove-twitter-100/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-100.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         )\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             )\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 100\n",
    "model_word2vec_glove_wiki_100 = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Let's use a model from Twitter corpora with embedding size of 100\n",
    "model_word2vec_glove_twitter_100 = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f65ddd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,complex,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-100\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 65\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8125\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,jurisdictions,wrong\n",
      "haphazardly,randomly,densely,wrong\n",
      "prominent,conspicuous,mysterious,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,endurance,correct\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,repeated,wrong\n",
      "constantly,continually,instantly,wrong\n",
      "issues,subjects,benefits,wrong\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,trade,wrong\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,election,wrong\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,relative,wrong\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,humorously,wrong\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unrecognized,wrong\n",
      "peculiarly,uniquely,partly,guess\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,overtly,wrong\n",
      "physician,doctor,nurse,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,materials,wrong\n",
      "unlikely,improbable,different,wrong\n",
      "halfheartedly,apathetically,apathetically,guess\n",
      "annals,chronicles,trails,wrong\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,prominent,wrong\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,binding,wrong\n",
      "tranquillity,peacefulness,weariness,wrong\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,consistently,wrong\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,equitable,wrong\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,profit,wrong\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,valuable,wrong\n",
      "fashion,manner,craze,wrong\n",
      "marketed,sold,diluted,wrong\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,permanently,wrong\n",
      "\n",
      "\n",
      "(a) Model name: glove-twitter-100\n",
      "(b) The size of the vocabulary: 1193514\n",
      "(c) The number of correct labels (call this 'C'): 39\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 78\n",
      "(e) the accuracy of the model (i.e. C/V): 0.5\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_100 = compute_synonyms(model_word2vec_glove_wiki_100, \"glove-wiki-gigaword-100\")\n",
    "performance_twitter_100 = compute_synonyms(model_word2vec_glove_twitter_100, \"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496b449",
   "metadata": {},
   "source": [
    "2. 2 new models from the same corpus but different embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6eb6083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 50\n",
    "model_word2vec_glove_wiki_50 = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Let's also use a model from Wikipedia corpora with embedding size of 300 instead\n",
    "model_word2vec_glove_wiki_300 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026e5bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,circulate,correct\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,pharmacist,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,isolated,wrong\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,unpopular,wrong\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,isolate,wrong\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,repeatedly,wrong\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,better,wrong\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-50\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 57\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.7125\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,supply,correct\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,circulate,correct\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,curved,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,ended,correct\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-300\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 71\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8875\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_50 = compute_synonyms(model_word2vec_glove_wiki_50, \"glove-wiki-gigaword-50\")\n",
    "performance_wiki_300 = compute_synonyms(model_word2vec_glove_wiki_300, \"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a62133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
