{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e25637",
   "metadata": {},
   "source": [
    "# 2.2 Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f24938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.8% 1659.9/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_word2vec_google_news_300 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1fc46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "def documentation(model):\n",
    "    # Let's print all the methods of the model\n",
    "    print(dir(model))\n",
    "\n",
    "documentation(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29611f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some testing to explore how different methods work!\n",
      "\n",
      "Let's test the following question_word to find the closest synonym: 'enormously'\n",
      "Given the following options: ['appropriately', 'uniquely', 'tremendously', 'decidedly']\n",
      "\n",
      "Using the 'model.most_similar_to_given' method, we compute the following: 'tremendously'\n",
      "\n",
      "Let's see the cosine between the 'enormously' and 'tremendously' using the 'model.similarity' method!\n",
      "The cosine similarity between enormously and tremendously is: 0.8185791969299316\n",
      "\n",
      "If we compute the cosine similarity manually, we should also achieve the same result: 0.8185791969299316\n"
     ]
    }
   ],
   "source": [
    "def testing(model): \n",
    "    print(\"Let's do some testing to explore how different methods work!\\n\")\n",
    "    \n",
    "    question_word = \"enormously\"\n",
    "    options = [\"appropriately\", \"uniquely\", \"tremendously\", \"decidedly\"]\n",
    "    print(f\"Let's test the following question_word to find the closest synonym: '{question_word}'\")\n",
    "    print(f\"Given the following options: {options}\")\n",
    "    \n",
    "    # Let's test the \"most_similar_to_given\" method!\n",
    "    answer_word = model.most_similar_to_given(question_word, options)\n",
    "    print(f\"\\nUsing the 'model.most_similar_to_given' method, we compute the following: '{answer_word}'\")\n",
    "    \n",
    "    # Let's test the \"model.similarity\" method!\n",
    "    print(f\"\\nLet's see the cosine between the '{question_word}' and '{answer_word}' using the 'model.similarity' method!\")\n",
    "    cos_sim = model.similarity(question_word, answer_word)\n",
    "    print(f\"The cosine similarity between {question_word} and {answer_word} is: {cos_sim}\")\n",
    "\n",
    "    # Let's make sure the answer computed above correctly calculates the cosine similarity! \n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    v1 = model.get_vector(question_word)\n",
    "    v2 = model.get_vector(answer_word)\n",
    "    cos_sim = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    print(f\"\\nIf we compute the cosine similarity manually, we should also achieve the same result: {cos_sim}\")\n",
    "\n",
    "testing(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1c35e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,ceremonies,wrong\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,impress,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,rear,correct\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,apathetically,correct\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,mastery,correct\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,happiness,guess\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,alike,correct\n",
      "figure,solve,solve,correct\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: word2vec-google-news-300\n",
      "(b) The size of the vocabulary: 3000000\n",
      "(c) The number of correct labels (call this 'C'): 70\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 79\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8860759493670886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.60759493670885"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "class Labels(Enum):\n",
    "    GUESS = \"guess\"\n",
    "    CORRECT = \"correct\"\n",
    "    WRONG = \"wrong\"\n",
    "    \n",
    "def random_guess(options):\n",
    "    random_index = random.randint(0, len(options) - 1)\n",
    "    return options[random_index]\n",
    "\n",
    "def print_analysis(model, model_name, num_correct, num_wrong):\n",
    "    print(f\"\\n\")\n",
    "    print(f\"(a) Model name: {model_name}\")\n",
    "    print(f\"(b) The size of the vocabulary: {len(model)}\")\n",
    "    print(f\"(c) The number of correct labels (call this 'C'): {num_correct}\")\n",
    "    print(f\"(d) The number of questions the Model answered without guessing (call this 'V' ): {num_correct + num_wrong}\")\n",
    "    print(f\"(e) the accuracy of the model (i.e. C/V): {num_correct/(num_correct + num_wrong)}\")\n",
    "\n",
    "def compute_most_similar_term(model, question_word, options):\n",
    "    similarity = None\n",
    "    guess_word = None\n",
    "    for option in options:\n",
    "        try:\n",
    "            current = model.similarity(question_word, option)\n",
    "            if not similarity or current > similarity:\n",
    "                similarity = current\n",
    "                guess_word = option\n",
    "        except KeyError:\n",
    "            pass     \n",
    "    return (guess_word, similarity)\n",
    "\n",
    "def compute_synonym(model, row):\n",
    "    question_word = row[0]\n",
    "    answer_word = row[1]\n",
    "    options = row[2:]\n",
    "    \n",
    "    guess_word, similarity, label = None, None, None\n",
    "    \n",
    "    # Let's check if the question word is defined in the vocabulary\n",
    "    try:\n",
    "        question_word_vector = model.get_vector(question_word)\n",
    "    except KeyError:\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "        return (question_word, answer_word, guess_word, similarity, Labels.GUESS)\n",
    "    \n",
    "    # If the question word is defined, we can proceed with finding the most similar term\n",
    "    (guess_word, similarity) = compute_most_similar_term(model, question_word, options)\n",
    "    \n",
    "    if guess_word == answer_word:\n",
    "        label = Labels.CORRECT\n",
    "    elif not guess_word:\n",
    "        # If none of the options are in the vocabulary, then it's a guess\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "    else:\n",
    "        label = Labels.WRONG\n",
    "    \n",
    "    return (question_word, answer_word, guess_word, similarity, label)\n",
    "            \n",
    "def compute_synonyms(model, model_name):\n",
    "    with open('synonyms.csv', newline='') as csvfile:\n",
    "\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        # Skip the first row of the csv\n",
    "        next(reader)\n",
    "        \n",
    "        num_correct, num_guess, num_wrong = 0, 0, 0\n",
    "\n",
    "        details_file = open(f\"{model_name}-details.csv\", mode='w', newline=\"\")\n",
    "        details_writer = csv.writer(details_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        analysis_file = open(f\"analysis.csv\", mode='a', newline=\"\")\n",
    "        analysis_writer = csv.writer(analysis_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for row in reader:\n",
    "            question_word, answer_word, guess_word, similarity, label = None, None, None, None, None\n",
    "\n",
    "            (question_word, answer_word, guess_word, similarity, label) = compute_synonym(model, row)\n",
    "            \n",
    "            if label == Labels.CORRECT:\n",
    "                num_correct += 1\n",
    "            elif label == Labels.GUESS:\n",
    "                num_guess += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            output = f\"{question_word},{answer_word},{guess_word},{label.value}\"\n",
    "            print(output)\n",
    "\n",
    "            details_writer.writerow([question_word, answer_word, guess_word, label.value])\n",
    "         \n",
    "        performance = (num_correct/(num_correct + num_wrong)) * 100\n",
    "        \n",
    "        analysis_writer.writerow([model_name, len(model), num_correct, (num_correct + num_wrong), performance])\n",
    "        print_analysis(model, model_name, num_correct, num_wrong)\n",
    "        \n",
    "        details_file.close()\n",
    "        analysis_file.close()\n",
    "        \n",
    "        return performance\n",
    "\n",
    "def clear_file(file_name):\n",
    "    analysis_file = open(f\"{file_name}\", mode='w')\n",
    "    analysis_file.truncate(0)\n",
    "    analysis_file.close()\n",
    "\n",
    "clear_file(\"analysis.csv\")\n",
    "clear_file(\"word2vec-google-news-300-details.csv\")\n",
    "compute_synonyms(model_word2vec_google_news_300, \"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0fd8d",
   "metadata": {},
   "source": [
    "# 2.3 Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4420d",
   "metadata": {},
   "source": [
    "1. 2 new models from different corpora but same embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f147f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the various models available for use in the Gensim library\n",
    "info = api.info()\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610515bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/qj3fdrhn4bl0x8qcjr71q42w0000gn/T/ipykernel_51519/2743817246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Let's use a model from Twitter corpora with embedding size of 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_word2vec_glove_twitter_100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-twitter-100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/glove-twitter-100/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-100.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         )\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             )\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 100\n",
    "model_word2vec_glove_wiki_100 = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Let's use a model from Twitter corpora with embedding size of 100\n",
    "model_word2vec_glove_twitter_100 = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5c0bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,complex,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-100\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 65\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8125\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,jurisdictions,wrong\n",
      "haphazardly,randomly,densely,wrong\n",
      "prominent,conspicuous,mysterious,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,endurance,correct\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,repeated,wrong\n",
      "constantly,continually,instantly,wrong\n",
      "issues,subjects,benefits,wrong\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,trade,wrong\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,election,wrong\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,relative,wrong\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,humorously,wrong\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unrecognized,wrong\n",
      "peculiarly,uniquely,patriotically,guess\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,overtly,wrong\n",
      "physician,doctor,nurse,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,materials,wrong\n",
      "unlikely,improbable,different,wrong\n",
      "halfheartedly,apathetically,customarily,guess\n",
      "annals,chronicles,trails,wrong\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,prominent,wrong\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,binding,wrong\n",
      "tranquillity,peacefulness,weariness,wrong\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,consistently,wrong\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,equitable,wrong\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,profit,wrong\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,valuable,wrong\n",
      "fashion,manner,craze,wrong\n",
      "marketed,sold,diluted,wrong\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,permanently,wrong\n",
      "\n",
      "\n",
      "(a) Model name: glove-twitter-100\n",
      "(b) The size of the vocabulary: 1193514\n",
      "(c) The number of correct labels (call this 'C'): 39\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 78\n",
      "(e) the accuracy of the model (i.e. C/V): 0.5\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_100 = compute_synonyms(model_word2vec_glove_wiki_100, \"glove-wiki-gigaword-100\")\n",
    "performance_twitter_100 = compute_synonyms(model_word2vec_glove_twitter_100, \"glove-twitter-100\")\n",
    "\n",
    "performances_list = [performance_wiki_100, performance_twitter_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67e9f518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's compare the results of models from different corpora but same embedding size\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "corpora_list = ['Twitter', 'Wikipedia']\n",
    "\n",
    "# Human gold standard performance retrieved from class results\n",
    "human_standard_perforance = 85.6\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.bar(corpora_list, performances_list, color = 'powderblue', width = 0.7, align = 'center')\n",
    "\n",
    "plt.xlabel(\"Corpora\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.title(\"Performance of Different Corpora with same Embedding Size\")\n",
    "plt.axhline(human_standard_perforance, color = 'orangered')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b18b2",
   "metadata": {},
   "source": [
    "2. 2 new models from the same corpus but different embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be9f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 50\n",
    "model_word2vec_glove_wiki_50 = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Let's also use a model from Wikipedia corpora with embedding size of 300 instead\n",
    "model_word2vec_glove_wiki_300 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91d854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,circulate,correct\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,pharmacist,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,isolated,wrong\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,unpopular,wrong\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,isolate,wrong\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,repeatedly,wrong\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,better,wrong\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-50\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 57\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.7125\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,supply,correct\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,circulate,correct\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,curved,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,ended,correct\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-300\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 71\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8875\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_50 = compute_synonyms(model_word2vec_glove_wiki_50, \"glove-wiki-gigaword-50\")\n",
    "performance_wiki_300 = compute_synonyms(model_word2vec_glove_wiki_300, \"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d77a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
