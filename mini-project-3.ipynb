{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e25637",
   "metadata": {},
   "source": [
    "# 2.2 Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f24938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domen\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_word2vec_google_news_300 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1fc46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "def documentation(model):\n",
    "    # Let's print all the methods of the model\n",
    "    print(dir(model))\n",
    "\n",
    "documentation(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29611f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some testing to explore how different methods work!\n",
      "\n",
      "Let's test the following question_word to find the closest synonym: 'enormously'\n",
      "Given the following options: ['appropriately', 'uniquely', 'tremendously', 'decidedly']\n",
      "\n",
      "Using the 'model.most_similar_to_given' method, we compute the following: 'tremendously'\n",
      "\n",
      "Let's see the cosine between the 'enormously' and 'tremendously' using the 'model.similarity' method!\n",
      "The cosine similarity between enormously and tremendously is: 0.8185791969299316\n",
      "\n",
      "If we compute the cosine similarity manually, we should also achieve the same result: 0.8185791969299316\n"
     ]
    }
   ],
   "source": [
    "def testing(model): \n",
    "    print(\"Let's do some testing to explore how different methods work!\\n\")\n",
    "    \n",
    "    question_word = \"enormously\"\n",
    "    options = [\"appropriately\", \"uniquely\", \"tremendously\", \"decidedly\"]\n",
    "    print(f\"Let's test the following question_word to find the closest synonym: '{question_word}'\")\n",
    "    print(f\"Given the following options: {options}\")\n",
    "    \n",
    "    # Let's test the \"most_similar_to_given\" method!\n",
    "    answer_word = model.most_similar_to_given(question_word, options)\n",
    "    print(f\"\\nUsing the 'model.most_similar_to_given' method, we compute the following: '{answer_word}'\")\n",
    "    \n",
    "    # Let's test the \"model.similarity\" method!\n",
    "    print(f\"\\nLet's see the cosine between the '{question_word}' and '{answer_word}' using the 'model.similarity' method!\")\n",
    "    cos_sim = model.similarity(question_word, answer_word)\n",
    "    print(f\"The cosine similarity between {question_word} and {answer_word} is: {cos_sim}\")\n",
    "\n",
    "    # Let's make sure the answer computed above correctly calculates the cosine similarity! \n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    v1 = model.get_vector(question_word)\n",
    "    v2 = model.get_vector(answer_word)\n",
    "    cos_sim = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    print(f\"\\nIf we compute the cosine similarity manually, we should also achieve the same result: {cos_sim}\")\n",
    "\n",
    "testing(model_word2vec_google_news_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c35e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,ceremonies,wrong\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,impress,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,rear,correct\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,apathetically,correct\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,mastery,correct\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,happiness,guess\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,alike,correct\n",
      "figure,solve,solve,correct\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: word2vec-google-news-300\n",
      "(b) The size of the vocabulary: 3000000\n",
      "(c) The number of correct labels (call this 'C'): 70\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 79\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8860759493670886\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "class Labels(Enum):\n",
    "    GUESS = \"guess\"\n",
    "    CORRECT = \"correct\"\n",
    "    WRONG = \"wrong\"\n",
    "    \n",
    "def random_guess(options):\n",
    "    random_index = random.randint(0, len(options) - 1)\n",
    "    return options[random_index]\n",
    "\n",
    "def print_analysis(model, model_name, num_correct, num_wrong):\n",
    "    print(f\"\\n\")\n",
    "    print(f\"(a) Model name: {model_name}\")\n",
    "    print(f\"(b) The size of the vocabulary: {len(model)}\")\n",
    "    print(f\"(c) The number of correct labels (call this 'C'): {num_correct}\")\n",
    "    print(f\"(d) The number of questions the Model answered without guessing (call this 'V' ): {num_correct + num_wrong}\")\n",
    "    print(f\"(e) the accuracy of the model (i.e. C/V): {num_correct/(num_correct + num_wrong)}\\n\\n\")\n",
    "\n",
    "def compute_most_similar_term(model, question_word, options):\n",
    "    similarity = None\n",
    "    guess_word = None\n",
    "    for option in options:\n",
    "        try:\n",
    "            current = model.similarity(question_word, option)\n",
    "            if not similarity or current > similarity:\n",
    "                similarity = current\n",
    "                guess_word = option\n",
    "        except KeyError:\n",
    "            pass     \n",
    "    return (guess_word, similarity)\n",
    "\n",
    "def compute_synonym(model, row):\n",
    "    question_word = row[0]\n",
    "    answer_word = row[1]\n",
    "    options = row[2:]\n",
    "    \n",
    "    guess_word, similarity, label = None, None, None\n",
    "    \n",
    "    # Let's check if the question word is defined in the vocabulary\n",
    "    try:\n",
    "        question_word_vector = model.get_vector(question_word)\n",
    "    except KeyError:\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "        return (question_word, answer_word, guess_word, similarity, Labels.GUESS)\n",
    "    \n",
    "    # If the question word is defined, we can proceed with finding the most similar term\n",
    "    (guess_word, similarity) = compute_most_similar_term(model, question_word, options)\n",
    "    \n",
    "    if guess_word == answer_word:\n",
    "        label = Labels.CORRECT\n",
    "    elif not guess_word:\n",
    "        # If none of the options are in the vocabulary, then it's a guess\n",
    "        label = Labels.GUESS\n",
    "        guess_word = random_guess(options)\n",
    "    else:\n",
    "        label = Labels.WRONG\n",
    "    \n",
    "    return (question_word, answer_word, guess_word, similarity, label)\n",
    "            \n",
    "def compute_synonyms(model, model_name):\n",
    "    with open('synonyms.csv', newline='') as csvfile:\n",
    "\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        # Skip the first row of the csv\n",
    "        next(reader)\n",
    "        \n",
    "        num_correct, num_guess, num_wrong = 0, 0, 0\n",
    "\n",
    "        details_file = open(f\"{model_name}-details.csv\", mode='w', newline=\"\")\n",
    "        details_writer = csv.writer(details_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        analysis_file = open(f\"analysis.csv\", mode='a', newline=\"\")\n",
    "        analysis_writer = csv.writer(analysis_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for row in reader:\n",
    "            question_word, answer_word, guess_word, similarity, label = None, None, None, None, None\n",
    "\n",
    "            (question_word, answer_word, guess_word, similarity, label) = compute_synonym(model, row)\n",
    "            \n",
    "            if label == Labels.CORRECT:\n",
    "                num_correct += 1\n",
    "            elif label == Labels.GUESS:\n",
    "                num_guess += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            output = f\"{question_word},{answer_word},{guess_word},{label.value}\"\n",
    "            print(output)\n",
    "\n",
    "            details_writer.writerow([question_word, answer_word, guess_word, label.value])\n",
    "         \n",
    "        performance = (num_correct/(num_correct + num_wrong)) * 100\n",
    "        \n",
    "        analysis_writer.writerow([model_name, len(model), num_correct, (num_correct + num_wrong), performance])\n",
    "        print_analysis(model, model_name, num_correct, num_wrong)\n",
    "        \n",
    "        details_file.close()\n",
    "        analysis_file.close()\n",
    "        \n",
    "        return performance\n",
    "\n",
    "def clear_file(file_name):\n",
    "    analysis_file = open(f\"{file_name}\", mode='w')\n",
    "    analysis_file.truncate(0)\n",
    "    analysis_file.close()\n",
    "\n",
    "clear_file(\"analysis.csv\")\n",
    "clear_file(\"word2vec-google-news-300-details.csv\")\n",
    "performance_google_300 = compute_synonyms(model_word2vec_google_news_300, \"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0fd8d",
   "metadata": {},
   "source": [
    "# 2.3 Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e576e67",
   "metadata": {},
   "source": [
    "**1) 2 new models from different corpora but same embedding size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116c2e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the various models available for use in the Gensim library\n",
    "info = api.info()\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9eb591b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 100\n",
    "model_glove_wiki_100 = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Let's use a model from Twitter corpora with embedding size of 100\n",
    "model_glove_twitter_100 = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55246b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,ancient,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,successful,wrong\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,unconventionally,wrong\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,peacefulness,correct\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,complex,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n",
      "\n",
      "\n",
      "(a) Model name: glove-wiki-gigaword-100\n",
      "(b) The size of the vocabulary: 400000\n",
      "(c) The number of correct labels (call this 'C'): 65\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 80\n",
      "(e) the accuracy of the model (i.e. C/V): 0.8125\n",
      "\n",
      "\n",
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,jurisdictions,wrong\n",
      "haphazardly,randomly,densely,wrong\n",
      "prominent,conspicuous,mysterious,wrong\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,greetings,correct\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,endurance,correct\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,repeated,wrong\n",
      "constantly,continually,instantly,wrong\n",
      "issues,subjects,benefits,wrong\n",
      "furnish,supply,advise,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,trade,wrong\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,election,wrong\n",
      "narrow,thin,clear,wrong\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,relative,wrong\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,humorously,wrong\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,promising,wrong\n",
      "unmatched,unequaled,unrecognized,wrong\n",
      "peculiarly,uniquely,uniquely,guess\n",
      "hue,color,color,correct\n",
      "hind,rear,hairy,wrong\n",
      "highlight,accentuate,alter,wrong\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,overtly,wrong\n",
      "physician,doctor,nurse,wrong\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,sharp,correct\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,materials,wrong\n",
      "unlikely,improbable,different,wrong\n",
      "halfheartedly,apathetically,unconventionally,guess\n",
      "annals,chronicles,trails,wrong\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,addressed,wrong\n",
      "command,mastery,observation,wrong\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,prominent,wrong\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,binding,wrong\n",
      "tranquillity,peacefulness,weariness,wrong\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,consistently,wrong\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,equitable,wrong\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,profit,wrong\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,hard,wrong\n",
      "figure,solve,list,wrong\n",
      "sufficient,enough,valuable,wrong\n",
      "fashion,manner,craze,wrong\n",
      "marketed,sold,diluted,wrong\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,permanently,wrong\n",
      "\n",
      "\n",
      "(a) Model name: glove-twitter-100\n",
      "(b) The size of the vocabulary: 1193514\n",
      "(c) The number of correct labels (call this 'C'): 39\n",
      "(d) The number of questions the Model answered without guessing (call this 'V' ): 78\n",
      "(e) the accuracy of the model (i.e. C/V): 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_wiki_100 = compute_synonyms(model_glove_wiki_100, \"glove-wiki-gigaword-100\")\n",
    "performance_twitter_100 = compute_synonyms(model_glove_twitter_100, \"glove-twitter-100\")\n",
    "\n",
    "performances_corpora_list = [performance_wiki_100, performance_twitter_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a2cd5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's compare the results of models from different corpora but same embedding size\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "corpora_list = ['Wikipedia (glove-wiki-gigaword-100)', 'Twitter (glove-twitter-100)']\n",
    "\n",
    "# Human gold standard performance retrieved from class results\n",
    "human_standard_perforance = 85.6\n",
    "\n",
    "# Average performance if choices were all guesses - on average, 1 correct answers out of 4 guess-words or 25%\n",
    "guess_performance = 25\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.bar(corpora_list, performances_corpora_list, color = 'lightblue', width = 0.7, align = 'center')\n",
    "\n",
    "plt.xlabel(\"Corpora\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.title(\"Performance of Different Corpora with Same Embedding Size\")\n",
    "plt.axhline(human_standard_perforance, label = 'Human Gold-Standard', color = 'orangered', lw = 3, ls = '--')\n",
    "plt.axhline(guess_performance, label = 'Only Guesses', color = 'green', lw = 3, ls = '--')\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c40e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the bar plot above, the performance of two different word embedding models, using Wikipedia and Twitter corpora, \n",
      "respectively, are compared. Upon seeing the results, it is evident that the Wikipedia model performs significantly\n",
      "better than the Twitter model with a performance rate of 81% versus only 50% for the Twitter one. The performance\n",
      "metric in this case relates to the number of correct synonym word answers over the number of questions the model \n",
      "answered without guessing. \n",
      "\n",
      "This difference in performance can be attributed to the informal nature of social media platforms such as Twitter\n",
      "where user's typically use slang and may have typos in their texts. This may, in turn, diminish the training \n",
      "quality since a word's window will contain this informal text. \n",
      "\n",
      "It is also worth noting that neither model performs better than the human gold-standard baseline. This can be due\n",
      "the optimal hyperparamters not being reached in either case leading to worse performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comparison_different_corpora = '''\n",
    "In the bar plot above, the performance of two different word embedding models, using Wikipedia and Twitter corpora, \n",
    "respectively, are compared. Upon seeing the results, it is evident that the Wikipedia model performs significantly\n",
    "better than the Twitter model with a performance rate of 81% versus only 50% for the Twitter one. The performance\n",
    "metric in this case relates to the number of correct synonym word answers over the number of questions the model \n",
    "answered without guessing. \n",
    "\n",
    "This difference in performance can be attributed to the informal nature of social media platforms such as Twitter\n",
    "where user's typically use slang and may have typos in their texts. This may, in turn, diminish the training \n",
    "quality since a word's window will contain this informal text. \n",
    "\n",
    "It is also worth noting that neither model performs better than the human gold-standard baseline. This can be due\n",
    "the optimal hyperparamters not being reached in either case leading to worse performance.\n",
    "'''\n",
    "print(comparison_different_corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d4508",
   "metadata": {},
   "source": [
    "**2) 2 new models from the same corpus but different embedding sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01f382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's use a model from Wikipedia corpora with embedding size of 50\n",
    "model_glove_wiki_50 = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Let's also use a model from Wikipedia corpora with embedding size of 300 instead\n",
    "model_glove_wiki_300 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_wiki_50 = compute_synonyms(model_glove_wiki_50, \"glove-wiki-gigaword-50\")\n",
    "performance_wiki_300 = compute_synonyms(model_glove_wiki_300, \"glove-wiki-gigaword-300\")\n",
    "\n",
    "performances_embedding_list = [performance_wiki_50,performance_wiki_100,performance_wiki_300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the results of models from same corpora but with different embedding size\n",
    "\n",
    "embedding_size_list = ['50 (glove-wiki-gigaword-50)','100 (glove-wiki-gigaword-100)','300 (glove-wiki-gigaword-300)']\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.bar(embedding_size_list, performances_embedding_list, color = 'lightblue', width = 0.6)\n",
    "plt.xticks(rotation = 20)\n",
    "\n",
    "plt.xlabel(\"Embedding Size\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.title(\"Performance of Wikipedia Corpora with Different Embedding Sizes\")\n",
    "plt.axhline(human_standard_perforance, label = 'Human Gold-Standard', color = 'orangered', lw = 3, ls = '--')\n",
    "plt.axhline(guess_performance, label = 'Only Guesses', color = 'green', lw = 3, ls = '--')\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_different_embedding_size = '''\n",
    "In the bar plot above, the performance of three word embedding models stemming from the same corpora\n",
    "but each with different embedding sizes are compared. It is evident that the model with the highest embedding size\n",
    "displayed the highest performance. This difference in performance can be due to a more optimal value for this\n",
    "hyper-parameter being reached when the embedding size was equal to 300. A higher dimensionality in this case may \n",
    "have been more optimal if the content of the corpora contained more information allowing for a higher \n",
    "dimensionality to capture greater lexical detail. \n",
    "\n",
    "Moreover, an embedding size of 300 allowed the model to perform at a greater level than the human gold-standard\n",
    "baseline of 85.6%.\n",
    "'''\n",
    "\n",
    "print(comparison_different_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f09621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the results of all models \n",
    "\n",
    "performances_all_models_list = [performance_google_300,performance_wiki_100,performance_twitter_100,performance_wiki_50,performance_wiki_300]\n",
    "model_names_list = [\"word2vec-google-news-300\",\"glove-wiki-gigaword-100\",\"glove-twitter-100\",\"glove-wiki-gigaword-50\",\"glove-wiki-gigaword-300\"]\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(model_names_list, performances_all_models_list, color = 'lightblue', width = 0.7)\n",
    "plt.xticks(rotation = 35)\n",
    "\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.title(\"Performance of Different Word Embedding Models\")\n",
    "plt.axhline(human_standard_perforance, label = 'Human Gold-Standard', color = 'orangered', lw = 3, ls = '--')\n",
    "plt.axhline(guess_performance, label = 'Only Guesses', color = 'green', lw = 3, ls = '--')\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_all_models = '''\n",
    "In the bar plot above, the performances of all models are compared. It is shown that the models from Google News \n",
    "and Wikipedia corpora, both with an embedding size of 300 performed the best with a rate of 88%. \n",
    "'''\n",
    "\n",
    "print(comparison_all_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
