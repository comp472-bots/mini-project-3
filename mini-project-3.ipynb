{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e25637",
   "metadata": {},
   "source": [
    "# 2.2 Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f24938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domen\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e326e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n",
      "tremendously\n"
     ]
    }
   ],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5290354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tremendously\n"
     ]
    }
   ],
   "source": [
    "# Let's test the \"most_similar_to_given\" method!\n",
    "print(model.most_similar_to_given(\"enormously\", [\"test\", \"tremendously\", \"unique\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f205755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tremendously correct\n",
      "stipulations correct\n",
      "randomly correct\n",
      "conspicuous correct\n",
      "pinnacle correct\n",
      "imperfect correct\n",
      "desperately correct\n",
      "eaten correct\n",
      "coming correct\n",
      "succinctly correct\n",
      "ceremonies wrong\n",
      "restless wrong\n",
      "accelerate correct\n",
      "generosity wrong\n",
      "imaginative correct\n",
      "demonstrated correct\n",
      "continually correct\n",
      "subjects correct\n",
      "impress wrong\n",
      "expensive correct\n",
      "acknowledged correct\n",
      "location correct\n",
      "earn correct\n",
      "frequently correct\n",
      "relaxed correct\n",
      "argument correct\n",
      "thin correct\n",
      "planned correct\n",
      "limitless correct\n",
      "prickly wrong\n",
      "imposed correct\n",
      "skillfully correct\n",
      "commercialize wrong\n",
      "differences correct\n",
      "productive correct\n",
      "unequaled correct\n",
      "uniquely correct\n",
      "color correct\n",
      "rear correct\n",
      "accentuate correct\n",
      "hurriedly correct\n",
      "mild correct\n",
      "smile correct\n",
      "orally correct\n",
      "doctor correct\n",
      "basically correct\n",
      "useful wrong\n",
      "positioned correct\n",
      "major correct\n",
      "gradually correct\n",
      "constructed correct\n",
      "jobs correct\n",
      "improbable correct\n",
      "apathetically correct\n",
      "chronicles correct\n",
      "furiously correct\n",
      "remembered wrong\n",
      "mastery correct\n",
      "devised correct\n",
      "potential correct\n",
      "broadly correct\n",
      "prolonged correct\n",
      "dangerous correct\n",
      "None guess\n",
      "disperse correct\n",
      "chiefly correct\n",
      "conversational correct\n",
      "settled correct\n",
      "possible correct\n",
      "rapidly correct\n",
      "proportion correct\n",
      "postponed wrong\n",
      "alike correct\n",
      "solve correct\n",
      "enough correct\n",
      "manner correct\n",
      "sold correct\n",
      "larger correct\n",
      "origins correct\n",
      "ordinarily correct\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('synonyms.csv', newline='') as csvfile:\n",
    "    \n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "    # Skip the first row of the csv\n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        word = row[0]\n",
    "        answer = row[1]\n",
    "        options = row[2:]\n",
    "        \n",
    "        guess_word = None\n",
    "        label = None\n",
    "        \n",
    "        try:\n",
    "            guess_word = model.most_similar_to_given(word, options)\n",
    "            label = \"correct\" if answer == guess_word else \"wrong\"\n",
    "        except KeyError:\n",
    "            label = \"guess\"\n",
    "        \n",
    "        print(guess_word, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0fd8d",
   "metadata": {},
   "source": [
    "# 2.3 Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f0c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53613fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
